{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISM6562 Final Exam - Programming Portion - Spring 2023\n",
    "\n",
    "In this programming portion of the exam, you will use PySpark to analyze a dataset. The dataset is provided in the file `transactions.csv`. The dataset contains transaction information for each sale at a given company.\n",
    "\n",
    "Instructions:\n",
    " 1. Rename this file to include your name. For example, if your name is John Smith, rename this file to \"John_Smith.ipynb\"\n",
    " 2. Complete the programming portion of the exam below.\n",
    "    1. There are 4 sections (7 questions in total).\n",
    "    2. Notice the time estimate provided for each section. You should use this as a guide to help you manage your time.\n",
    " 3. Submit your completed Jupyter Notebook file to Canvas along with the data your analyzed.\n",
    "\n",
    "The code you write should be well documented and easy to read. You should use comments to explain your code. \n",
    "\n",
    "The code required to complete this exam is very similar to the example PySpark code provided in the course. You can refer to this code as well as your own code stored on your computer or in your github repo.\n",
    "\n",
    "**IMPORTANT***: You can use Jupyter Notebook, Jupyter Lab, or VS Code. Though you can also use VS Code. You must not have CodePilot, ChatGP, or any other plugins/software that provides code suggestions or auto-completion. You must write all of the code yourself.\n",
    "\n",
    "**IMPORTANT: You should not share your exam with anyone else. Doing so will be considered a violation of the USF Academic Misconduct Policy.**\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start The Spark section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark App01\").enableHiveSupport().getOrCreate();\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext  \n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)\n",
    "\n",
    "# It's best if you find that the port number displayed below is not 4040, then you should shut down all other spark sessions and \n",
    "# run this code again. If you don't, you may have trouble accessing the data in the spark-warehouse directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.226.63.199:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 Spark App01</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1858e93dbd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will set the log level to ERROR. This will hide the INFO or WARNING messages that are printed out by default. If you want to see them, set this to INFO or WARN.\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load data - 0.5 pts (estimated time <5 minutes)\n",
    "\n",
    "Load the data you have been given (see question in canvas) into a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-----------+-----------+----------+---------+--------------------+--------------+-----+--------------------+----------------+--------------------+--------------------+----------+--------+\n",
      "|      transaction_id|      date|    time|day_of_week|customer_id|first_name|last_name|               email|         state|  zip|              street|            city|               phone|             product|sale_price|returned|\n",
      "+--------------------+----------+--------+-----------+-----------+----------+---------+--------------------+--------------+-----+--------------------+----------------+--------------------+--------------------+----------+--------+\n",
      "|294e63f6-c6d3-439...|2018-07-27|05:13:37|     Friday|      19710|    Amanda|    Weber|brichardson@examp...|          Iowa|62736|     292 Jones Hills|    South Joseph|  (497)740-6101x1146|Total grid-enable...|     61.18|   false|\n",
      "|46882d27-7d12-4e1...|2021-09-11|13:57:48|   Saturday|      31066| Frederick|     Hart|coxjessica@exampl...|       Wyoming| 3254|    76490 White Cove|     Charlesfort|          1396078600|Networked fault-t...|     137.2|    true|\n",
      "|3dbccb99-55a3-458...|2020-05-18|03:27:06|     Monday|      73764|      Ryan|   Walker|brendalee@example...|        Hawaii|47677|1990 Taylor Under...|       Bethville|    204.484.5016x460|Intuitive executi...|     30.38|   false|\n",
      "|f2dcd898-8864-485...|2021-04-26|03:09:05|     Monday|      53467|     Vicki|   Cannon|michaeljames@exam...|      Kentucky|33860|  06621 Calvin River| North Jeffshire| +1-558-882-5826x627|Vision-oriented r...|    135.03|    true|\n",
      "|8cb93755-e5c9-4c5...|2021-07-02|16:20:29|     Friday|      89845|     Jason|      Cox|lawrencescott@exa...|        Oregon| 5635| 5594 Richard Square|     Powellhaven|001-545-820-1574x...|Advanced bottom-l...|     75.12|    true|\n",
      "|ae647309-a4e9-494...|2020-01-31|05:48:54|     Friday|      90972|     Sarah|   Walker|  tmason@example.net|       Arizona|63936|54950 Edward Way ...|    Jonesborough|   (542)447-7229x779|Right-sized eco-c...|     85.27|    true|\n",
      "|fa92bf78-9656-45e...|2021-03-29|13:56:05|     Monday|      83900|  Victoria|    Miles|leonardkimberly@e...|          Ohio|52187|   152 Powell Tunnel|      New Lauren|  781.166.3409x17172|Persevering optim...|    125.26|    true|\n",
      "|4f47dbb8-04d5-403...|2021-05-12|21:20:46|  Wednesday|      18350|     Anita|    Henry|downsvincent@exam...|    Washington|40557|60022 Levine Fort...| East Thomasberg|       (001)512-3082|Diverse solution-...|     87.94|    true|\n",
      "|4cb4e738-040c-4eb...|2021-09-09|04:31:44|   Thursday|      68457|      Mike|   Thomas|wilsonkaren@examp...|       Arizona|47916|91435 Williams Burgs|     New Timothy|   961-929-9256x6112|Total human-resou...|     49.69|   false|\n",
      "|b21c443d-a75a-493...|2021-11-27|21:17:51|   Saturday|      12497|      Mark|    Baker| james26@example.net|    Washington|59217|   152 Timothy Manor|     Chelseyport|     +1-620-103-6587|Open-source 4thge...|    108.09|    true|\n",
      "|a739cc6f-0fd8-40a...|2022-02-01|16:56:59|    Tuesday|      73193|     Amber|   Lawson|hickmanphilip@exa...| New Hampshire|93791|806 Washington Fl...|       Perryside|   277.318.5068x0951|Programmable well...|    116.28|    true|\n",
      "|fea6db1c-605d-4bc...|2019-06-11|20:47:58|    Tuesday|      48831|   Timothy|   Taylor|alexis08@example.net|       Arizona|50984|7285 Vazquez Hill...|     Erikchester|        185.933.8585|Integrated intang...|     44.14|   false|\n",
      "|66191774-eba0-4f8...|2020-04-09|09:41:29|   Thursday|      16422|      Jose|  Jackson| xwalton@example.net|     Tennessee|17854|64520 Vasquez Fal...|     Brandihaven|   (103)008-7850x009|Upgradable full-r...|    106.72|    true|\n",
      "|06e806e6-ed1a-48d...|2019-03-31|04:15:27|     Sunday|      73193|     Amber|   Lawson|hickmanphilip@exa...| New Hampshire|93791|806 Washington Fl...|       Perryside|   277.318.5068x0951|Fully-configurabl...|     96.95|    true|\n",
      "|31b4fca0-fe85-4db...|2021-04-24|14:38:13|   Saturday|      21316|     Glenn|   Austin|robert20@example.org|       Florida| 4690|896 William Overpass|      Port Jamie|  877-669-8152x21743|Quality-focused h...|     55.44|   false|\n",
      "|51dbffdb-d46e-40e...|2020-04-04|04:24:48|   Saturday|      31264|   Matthew|   Hoover|edwardsjuan@examp...|         Idaho|73316| 49957 Shaffer Flats|  New Gloriafurt|    941-618-3233x522|Multi-tiered tang...|    103.36|    true|\n",
      "|0a3a7533-a7e9-41b...|2022-05-28|18:25:59|   Saturday|      83482|    Lauren|  Chapman|hinesbrian@exampl...|        Hawaii|26629|3457 Melanie Hill...|     Charlesbury|001-618-995-4759x802|Inverse 4thgenera...|     44.47|   false|\n",
      "|c9db60f9-5fb5-408...|2023-01-14|14:57:39|   Saturday|      76201|     Robin|    Roman|mitchellsusan@exa...|  Pennsylvania|98541|   0271 Heidi Estate|       North Amy|   222-487-3092x8297|Progressive terti...|     72.12|    true|\n",
      "|97d14123-fe01-46f...|2019-07-11|05:04:41|   Thursday|      65169|    Morgan|     Long|gillzachary@examp...|        Alaska|17514|78383 Johnson Cen...|East Richardtown|        105-291-7370|Self-enabling enc...|    100.42|    true|\n",
      "|e6163b8b-0365-41c...|2021-05-07|12:36:16|     Friday|      35707|    Carrie|    Salas|schultzkatrina@ex...|North Carolina|81980|88966 Taylor Lock...| West Curtisstad|       (530)536-7453|Integrated multim...|     20.11|   false|\n",
      "+--------------------+----------+--------+-----------+-----------+----------+---------+--------------------+--------------+-----+--------------------+----------------+--------------------+--------------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# insert code here\n",
    "trans_df=spark.read.csv(r\"C:\\Users\\rmura\\BigData\\week 8\\transactions.csv\",header=True, inferSchema=True)\n",
    "trans_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- sale_price: double (nullable = true)\n",
      " |-- returned: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Transform Data - total 1 pts (estimated time <10 minutes)\n",
    "\n",
    "Conduct any data tramsformations necessary (i.e. data cleaning, data type conversions, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp\n",
    "import pyspark.sql.functions as f\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    " \n",
    "# insert code here\n",
    "trans_df=trans_df.withColumn('date',to_date(trans_df.transaction_id, 'dd/MM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.createOrReplaceTempView(\"trans_tempView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|         |trans_tempview|       true|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables=spark.sql(\"show tables\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Explore Data - total 3 pts (estimated time <15 minutes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What day of the week has the highest number of sales? (1.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">transaction_id</td><td style=\"font-weight: bold\">date</td><td style=\"font-weight: bold\">time</td><td style=\"font-weight: bold\">day_of_week</td><td style=\"font-weight: bold\">customer_id</td><td style=\"font-weight: bold\">first_name</td><td style=\"font-weight: bold\">last_name</td><td style=\"font-weight: bold\">email</td><td style=\"font-weight: bold\">state</td><td style=\"font-weight: bold\">zip</td><td style=\"font-weight: bold\">street</td><td style=\"font-weight: bold\">city</td><td style=\"font-weight: bold\">phone</td><td style=\"font-weight: bold\">product</td><td style=\"font-weight: bold\">sale_price</td><td style=\"font-weight: bold\">returned</td></tr><tr><td>294e63f6-c6d3-4392-9991-29513874fbbd</td><td>null</td><td>05:13:37</td><td>Friday</td><td>19710</td><td>Amanda</td><td>Weber</td><td>brichardson@example.org</td><td>Iowa</td><td>62736</td><td>292 Jones Hills</td><td>South Joseph</td><td>(497)740-6101x1146</td><td>Total grid-enabled Graphical User Interface</td><td>61.18</td><td>False</td></tr><tr><td>46882d27-7d12-4e1d-8aab-929fba924945</td><td>null</td><td>13:57:48</td><td>Saturday</td><td>31066</td><td>Frederick</td><td>Hart</td><td>coxjessica@example.com</td><td>Wyoming</td><td>3254</td><td>76490 White Cove</td><td>Charlesfort</td><td>1396078600</td><td>Networked fault-tolerant standardization</td><td>137.2</td><td>True</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%sparksql\n",
    "select  * from trans_tempview limit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+------------+-----+--------------------+-----------------+-------------------+--------------------+----------+--------+\n",
      "|      transaction_id|date|    time|day_of_week|customer_id|first_name|last_name|               email|       state|  zip|              street|             city|              phone|             product|sale_price|returned|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+------------+-----+--------------------+-----------------+-------------------+--------------------+----------+--------+\n",
      "|922efe47-36e0-49e...|null|12:19:02|    Tuesday|      40999| Stephanie|    Combs|spearstammy@examp...|South Dakota|28110|56764 Joshua Village|South Dawnchester|+1-673-405-4676x100|Switchable optimi...|    147.83|    true|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+------------+-----+--------------------+-----------------+-------------------+--------------------+----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dg=trans_df.sort(\"sale_price\",ascending=False).show(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got high sales from table in week of Tuesady "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Which state has the highest total dollar value of sales? (1.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|       total_price|  state|\n",
      "+------------------+-------+\n",
      "|4067.4700000000007|Arizona|\n",
      "+------------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# insert code here\n",
    "trans_df.groupBy('state',)\n",
    "spark.sql(\" select sum(sale_price) as total_price,state from trans_tempview group by state order by total_price desc limit 5\").show(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got arizon as highest total dollars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Model the data - 5.5 total pts (estimated time <30 minutes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Develop Predictive Model (2.5 pts)\n",
    "\n",
    "Develop a predictive model that will predict if a sale will result in a return. The input variables/features for this model are sale price, discount, and day of week. Split your data into a train/test split of .7/.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+--------+-----+--------------------+------------+------------------+--------------------+----------+--------+\n",
      "|      transaction_id|date|    time|day_of_week|customer_id|first_name|last_name|               email|   state|  zip|              street|        city|             phone|             product|sale_price|returned|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+--------+-----+--------------------+------------+------------------+--------------------+----------+--------+\n",
      "|0070e01b-0d75-4f6...|null|18:27:52|     Monday|      23312|   Michael|    Evans|  troy48@example.org|Illinois|34660|45501 Robert Caus...|Lake Anntown| 878-741-7795x9438|Sharable content-...|     89.67|    true|\n",
      "|00d35181-cf15-402...|null|10:40:02|  Wednesday|      98465|     Brian|   Larson|darrenbenton@exam...|    Iowa|69243|8628 Butler Park ...|  Port Megan|(165)459-4732x6261|Persistent attitu...|     33.14|   false|\n",
      "|00f18192-3d29-42a...|null|06:12:25|     Sunday|      89357|    Philip|     Bell|hernandeztimothy@...|Michigan|14201| 5545 Michael Plains| Thomasmouth|      528.207.6048|Pre-emptive backg...|     101.3|    true|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+--------+-----+--------------------+------------+------------------+--------------------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# insert code here\n",
    "train_data,test_data=trans_df.randomSplit([0.7,0.3])\n",
    "train_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.select of DataFrame[transaction_id: string, date: date, time: string, day_of_week: string, customer_id: int, first_name: string, last_name: string, email: string, state: string, zip: int, street: string, city: string, phone: string, product: string, sale_price: double, returned: boolean]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|day_of_week|sale_price|\n",
      "+-----------+----------+\n",
      "|     Monday|     89.67|\n",
      "|  Wednesday|     33.14|\n",
      "|     Sunday|     101.3|\n",
      "+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfg=train_data.select(['day_of_week','sale_price'])\n",
    "dfg.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week_4 = StringIndexer(inputCol='day_of_week',outputCol='day_of_week_3',handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Vector assembler is used to create a vector of input features\n",
    " \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"day_of_week_3\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[\n",
    "    day_of_week_4,\n",
    "    assembler\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pipe=pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+-------------+-----+--------------------+---------------+--------------------+--------------------+----------+--------+-------------+--------+\n",
      "|      transaction_id|date|    time|day_of_week|customer_id|first_name|last_name|               email|        state|  zip|              street|           city|               phone|             product|sale_price|returned|day_of_week_3|features|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+-------------+-----+--------------------+---------------+--------------------+--------------------+----------+--------+-------------+--------+\n",
      "|0070e01b-0d75-4f6...|null|18:27:52|     Monday|      23312|   Michael|    Evans|  troy48@example.org|     Illinois|34660|45501 Robert Caus...|   Lake Anntown|   878-741-7795x9438|Sharable content-...|     89.67|    true|          0.0|   [0.0]|\n",
      "|00d35181-cf15-402...|null|10:40:02|  Wednesday|      98465|     Brian|   Larson|darrenbenton@exam...|         Iowa|69243|8628 Butler Park ...|     Port Megan|  (165)459-4732x6261|Persistent attitu...|     33.14|   false|          3.0|   [3.0]|\n",
      "|00f18192-3d29-42a...|null|06:12:25|     Sunday|      89357|    Philip|     Bell|hernandeztimothy@...|     Michigan|14201| 5545 Michael Plains|    Thomasmouth|        528.207.6048|Pre-emptive backg...|     101.3|    true|          1.0|   [1.0]|\n",
      "|0128b706-db33-44a...|null|14:26:33|     Friday|      83900|  Victoria|    Miles|leonardkimberly@e...|         Ohio|52187|   152 Powell Tunnel|     New Lauren|  781.166.3409x17172|Digitized actuati...|     126.4|    true|          2.0|   [2.0]|\n",
      "|01854f8c-3067-4c1...|null|11:35:14|    Tuesday|      88640|   Theresa|   Brooks|michellehernandez...|Massachusetts|98298|001 Baker Circle ...|   Stoneborough|+1-954-509-9631x6187|Sharable zero tol...|    125.19|    true|          5.0|   [5.0]|\n",
      "|022d0bf3-85fa-417...|null|07:36:53|     Friday|      15337| Gabrielle|     Wong|millerkatherine@e...|         Utah|41287|6535 Laura Dam Ap...|       Mariaton|    001-620-251-6597|Streamlined dedic...|      87.3|    true|          2.0|   [2.0]|\n",
      "|02f963de-e4f8-4c7...|null|02:23:34|   Saturday|      26868|      Mark|   Little|allison90@example...|      Montana|68186|61687 Gordon Plai...|Lake Douglaston|        616.548.6231|Decentralized enc...|    106.47|    true|          6.0|   [6.0]|\n",
      "|037659ae-f7fa-45b...|null|00:59:51|    Tuesday|      21316|     Glenn|   Austin|robert20@example.org|      Florida| 4690|896 William Overpass|     Port Jamie|  877-669-8152x21743|Optimized tertiar...|     70.04|   false|          5.0|   [5.0]|\n",
      "|037a2460-50b4-438...|null|02:04:02|  Wednesday|      17002|     Tracy|    Davis|anthony48@example...|      Arizona|23256|   79682 Robert View|   Barbaraburgh|        351-192-6775|Mandatory maximiz...|     13.28|   false|          3.0|   [3.0]|\n",
      "|039ec0cc-5b9a-450...|null|04:13:20|     Sunday|      98465|     Brian|   Larson|darrenbenton@exam...|         Iowa|69243|8628 Butler Park ...|     Port Megan|  (165)459-4732x6261|Monitored disinte...|     77.58|    true|          1.0|   [1.0]|\n",
      "|03c53f7d-39e6-407...|null|03:25:56|     Monday|      78693|  Kimberly|   Hansen| mandy63@example.net|       Alaska|58357|591 King Brook Su...|       Ballview|        204.992.5162|Ergonomic respons...|     102.3|    true|          0.0|   [0.0]|\n",
      "|04897428-dba1-482...|null|09:33:40|    Tuesday|      25399|  Danielle|  Pearson|jstrickland@examp...|         Utah|99315|3415 Robert Junction| New Howardtown|+1-812-907-5484x7...|Programmable well...|     84.22|    true|          5.0|   [5.0]|\n",
      "|04cfca45-7378-415...|null|22:43:39|   Thursday|      24937|   Abigail|    Smith|stephenkramer@exa...|     Maryland|80180|3446 Justin Coves...| East Elizabeth|        015.591.5145|Robust human-reso...|     86.58|    true|          4.0|   [4.0]|\n",
      "|04e05bcd-c95d-4bc...|null|13:42:11|   Thursday|      90347|     David|    Garza|velazquezbrenda@e...|     New York|74643|150 Samantha Moun...|North Scotttown|     +1-253-991-5764|Sharable web-enab...|     73.63|    true|          4.0|   [4.0]|\n",
      "|04f61d01-727c-410...|null|17:18:44|     Friday|      63273|      Ryan|    Baker|andersonjeremy@ex...|     New York|32960| 6186 Lindsey Stream| Stephaniemouth|        348-089-0465|Open-architected ...|     40.15|   false|          2.0|   [2.0]|\n",
      "|058baf2f-bf58-477...|null|15:20:57|     Monday|      11632|    Nathan|   Martin| amber68@example.com|    Louisiana|13497|6243 Ryan Extensions|New Cherylshire|        361.433.1535|Total non-volatil...|     15.83|   false|          0.0|   [0.0]|\n",
      "|05ca947d-fc4a-43c...|null|16:44:05|    Tuesday|      68882|    Sharon|     Cruz|justinreese@examp...|      Alabama|10596|6556 Alexander Cliff|     Weaverfort| +1-403-963-8396x928|Innovative system...|     86.05|    true|          5.0|   [5.0]|\n",
      "|05f29de4-4b62-42f...|null|14:01:49|     Monday|      83900|  Victoria|    Miles|leonardkimberly@e...|         Ohio|52187|   152 Powell Tunnel|     New Lauren|  781.166.3409x17172|Adaptive neutral ...|     39.95|   false|          0.0|   [0.0]|\n",
      "|060c3b5c-7c68-4e4...|null|16:26:59|   Saturday|      47865|     Kelly|   Wright|melissajennings@e...|    Minnesota|85863|467 Wyatt Station...|     South Anna|        859.200.1146|Phased clear-thin...|     60.28|   false|          6.0|   [6.0]|\n",
      "|06132007-2e45-415...|null|07:45:28|     Sunday|      16422|      Jose|  Jackson| xwalton@example.net|    Tennessee|17854|64520 Vasquez Fal...|    Brandihaven|   (103)008-7850x009|Distributed recip...|     38.19|   false|          1.0|   [1.0]|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+-------------+-----+--------------------+---------------+--------------------+--------------------+----------+--------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data=fitted_pipe.transform(train_data)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(labelCol=\"sale_price\")\n",
    "fit_model = lr_model.fit(train_data.select(['features','sale_price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+-------------+-----+--------------------+---------------+--------------------+--------------------+----------+--------+-------------+--------+-----------------+\n",
      "|      transaction_id|date|    time|day_of_week|customer_id|first_name|last_name|               email|        state|  zip|              street|           city|               phone|             product|sale_price|returned|day_of_week_3|features|       prediction|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+-------------+-----+--------------------+---------------+--------------------+--------------------+----------+--------+-------------+--------+-----------------+\n",
      "|0070e01b-0d75-4f6...|null|18:27:52|     Monday|      23312|   Michael|    Evans|  troy48@example.org|     Illinois|34660|45501 Robert Caus...|   Lake Anntown|   878-741-7795x9438|Sharable content-...|     89.67|    true|          0.0|   [0.0]|68.75988325686536|\n",
      "|00d35181-cf15-402...|null|10:40:02|  Wednesday|      98465|     Brian|   Larson|darrenbenton@exam...|         Iowa|69243|8628 Butler Park ...|     Port Megan|  (165)459-4732x6261|Persistent attitu...|     33.14|   false|          3.0|   [3.0]|69.77259156079485|\n",
      "|00f18192-3d29-42a...|null|06:12:25|     Sunday|      89357|    Philip|     Bell|hernandeztimothy@...|     Michigan|14201| 5545 Michael Plains|    Thomasmouth|        528.207.6048|Pre-emptive backg...|     101.3|    true|          1.0|   [1.0]|69.09745269150852|\n",
      "|0128b706-db33-44a...|null|14:26:33|     Friday|      83900|  Victoria|    Miles|leonardkimberly@e...|         Ohio|52187|   152 Powell Tunnel|     New Lauren|  781.166.3409x17172|Digitized actuati...|     126.4|    true|          2.0|   [2.0]|69.43502212615168|\n",
      "|01854f8c-3067-4c1...|null|11:35:14|    Tuesday|      88640|   Theresa|   Brooks|michellehernandez...|Massachusetts|98298|001 Baker Circle ...|   Stoneborough|+1-954-509-9631x6187|Sharable zero tol...|    125.19|    true|          5.0|   [5.0]|70.44773043008117|\n",
      "|022d0bf3-85fa-417...|null|07:36:53|     Friday|      15337| Gabrielle|     Wong|millerkatherine@e...|         Utah|41287|6535 Laura Dam Ap...|       Mariaton|    001-620-251-6597|Streamlined dedic...|      87.3|    true|          2.0|   [2.0]|69.43502212615168|\n",
      "|02f963de-e4f8-4c7...|null|02:23:34|   Saturday|      26868|      Mark|   Little|allison90@example...|      Montana|68186|61687 Gordon Plai...|Lake Douglaston|        616.548.6231|Decentralized enc...|    106.47|    true|          6.0|   [6.0]|70.78529986472432|\n",
      "|037659ae-f7fa-45b...|null|00:59:51|    Tuesday|      21316|     Glenn|   Austin|robert20@example.org|      Florida| 4690|896 William Overpass|     Port Jamie|  877-669-8152x21743|Optimized tertiar...|     70.04|   false|          5.0|   [5.0]|70.44773043008117|\n",
      "|037a2460-50b4-438...|null|02:04:02|  Wednesday|      17002|     Tracy|    Davis|anthony48@example...|      Arizona|23256|   79682 Robert View|   Barbaraburgh|        351-192-6775|Mandatory maximiz...|     13.28|   false|          3.0|   [3.0]|69.77259156079485|\n",
      "|039ec0cc-5b9a-450...|null|04:13:20|     Sunday|      98465|     Brian|   Larson|darrenbenton@exam...|         Iowa|69243|8628 Butler Park ...|     Port Megan|  (165)459-4732x6261|Monitored disinte...|     77.58|    true|          1.0|   [1.0]|69.09745269150852|\n",
      "|03c53f7d-39e6-407...|null|03:25:56|     Monday|      78693|  Kimberly|   Hansen| mandy63@example.net|       Alaska|58357|591 King Brook Su...|       Ballview|        204.992.5162|Ergonomic respons...|     102.3|    true|          0.0|   [0.0]|68.75988325686536|\n",
      "|04897428-dba1-482...|null|09:33:40|    Tuesday|      25399|  Danielle|  Pearson|jstrickland@examp...|         Utah|99315|3415 Robert Junction| New Howardtown|+1-812-907-5484x7...|Programmable well...|     84.22|    true|          5.0|   [5.0]|70.44773043008117|\n",
      "|04cfca45-7378-415...|null|22:43:39|   Thursday|      24937|   Abigail|    Smith|stephenkramer@exa...|     Maryland|80180|3446 Justin Coves...| East Elizabeth|        015.591.5145|Robust human-reso...|     86.58|    true|          4.0|   [4.0]|70.11016099543801|\n",
      "|04e05bcd-c95d-4bc...|null|13:42:11|   Thursday|      90347|     David|    Garza|velazquezbrenda@e...|     New York|74643|150 Samantha Moun...|North Scotttown|     +1-253-991-5764|Sharable web-enab...|     73.63|    true|          4.0|   [4.0]|70.11016099543801|\n",
      "|04f61d01-727c-410...|null|17:18:44|     Friday|      63273|      Ryan|    Baker|andersonjeremy@ex...|     New York|32960| 6186 Lindsey Stream| Stephaniemouth|        348-089-0465|Open-architected ...|     40.15|   false|          2.0|   [2.0]|69.43502212615168|\n",
      "|058baf2f-bf58-477...|null|15:20:57|     Monday|      11632|    Nathan|   Martin| amber68@example.com|    Louisiana|13497|6243 Ryan Extensions|New Cherylshire|        361.433.1535|Total non-volatil...|     15.83|   false|          0.0|   [0.0]|68.75988325686536|\n",
      "|05ca947d-fc4a-43c...|null|16:44:05|    Tuesday|      68882|    Sharon|     Cruz|justinreese@examp...|      Alabama|10596|6556 Alexander Cliff|     Weaverfort| +1-403-963-8396x928|Innovative system...|     86.05|    true|          5.0|   [5.0]|70.44773043008117|\n",
      "|05f29de4-4b62-42f...|null|14:01:49|     Monday|      83900|  Victoria|    Miles|leonardkimberly@e...|         Ohio|52187|   152 Powell Tunnel|     New Lauren|  781.166.3409x17172|Adaptive neutral ...|     39.95|   false|          0.0|   [0.0]|68.75988325686536|\n",
      "|060c3b5c-7c68-4e4...|null|16:26:59|   Saturday|      47865|     Kelly|   Wright|melissajennings@e...|    Minnesota|85863|467 Wyatt Station...|     South Anna|        859.200.1146|Phased clear-thin...|     60.28|   false|          6.0|   [6.0]|70.78529986472432|\n",
      "|06132007-2e45-415...|null|07:45:28|     Sunday|      16422|      Jose|  Jackson| xwalton@example.net|    Tennessee|17854|64520 Vasquez Fal...|    Brandihaven|   (103)008-7850x009|Distributed recip...|     38.19|   false|          1.0|   [1.0]|69.09745269150852|\n",
      "+--------------------+----+--------+-----------+-----------+----------+---------+--------------------+-------------+-----+--------------------+---------------+--------------------+--------------------+----------+--------+-------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = fit_model.transform(train_data)\n",
    "results.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Predictive Model (1.5 pts)\n",
    "\n",
    "Evaluate the model using a test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: transaction_id, date, time, day_of_week, customer_id, first_name, last_name, email, state, zip, street, city, phone, product, sale_price, returned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_d \u001b[39m=\u001b[39m fit_model\u001b[39m.\u001b[39;49mtransform(test_data)\n\u001b[0;32m      2\u001b[0m test_d\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\ml\\base.py:217\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[0;32m    216\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[0;32m    218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\ml\\wrapper.py:350\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 350\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mtransform(dataset\u001b[39m.\u001b[39;49m_jdf), dataset\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: transaction_id, date, time, day_of_week, customer_id, first_name, last_name, email, state, zip, street, city, phone, product, sale_price, returned"
     ]
    }
   ],
   "source": [
    "test_d = fit_model.transform(test_data)\n",
    "test_d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: transaction_id, date, time, day_of_week, customer_id, first_name, last_name, email, state, zip, street, city, phone, product, sale_price, returned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_results \u001b[39m=\u001b[39m fit_model\u001b[39m.\u001b[39;49mevaluate(test_data)\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\ml\\regression.py:375\u001b[0m, in \u001b[0;36mLinearRegressionModel.evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset, DataFrame):\n\u001b[0;32m    374\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdataset must be a DataFrame but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(dataset))\n\u001b[1;32m--> 375\u001b[0m java_lr_summary \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_java(\u001b[39m\"\u001b[39;49m\u001b[39mevaluate\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset)\n\u001b[0;32m    376\u001b[0m \u001b[39mreturn\u001b[39;00m LinearRegressionSummary(java_lr_summary)\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\ml\\wrapper.py:54\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[1;34m(self, name, *args)\u001b[0m\n\u001b[0;32m     52\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[0;32m     53\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, m(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\rmura\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: transaction_id, date, time, day_of_week, customer_id, first_name, last_name, email, state, zip, street, city, phone, product, sale_price, returned"
     ]
    }
   ],
   "source": [
    "test_results = fit_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n",
    "print(f\"{'Ex Var:':7s} {test_results.explainedVariance:>7.3f}\")\n",
    "print(f\"{'MAE:':7s} {test_results.meanAbsoluteError:>7.3f}\")\n",
    "print(f\"{'MSE:':7s} {test_results.meanSquaredError:>7.3f}\")\n",
    "print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n",
    "print(f\"{'R2:':7s} {test_results.r2:>7.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prediction (1.5 pts)\n",
    "\n",
    "Use the model you developed to predict if the following sales will result in a return:\n",
    "\n",
    "| Sales Price | Discount | Day of Week |\n",
    "|-------------|----------|-------------|\n",
    "| 100         | 0.0      | 1           |\n",
    "| 200         | 0.1      | 2           |\n",
    "| 300         | 0.1      | 3           |\n",
    "| 400         | 0.3      | 4           |\n",
    "| 500         | 0.2      | 5           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code Here\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analaysis\n",
    "\n",
    "- First we download the data nd read the data using pyspark \n",
    "- after reading the data we need to check the data properties\n",
    "- and run required quires based on requirmnet \n",
    "- vector assemle and piplein ethe data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
